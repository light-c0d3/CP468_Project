{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/light-c0d3/CP468_Project/blob/main/CP468_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CP468 Project\n",
        "\n",
        "Tanya Yousofi,\n",
        "Ruveyda Nur Kizmaz,\n",
        "Zahra Mohamed,\n",
        "Malika Sharma\n"
      ],
      "metadata": {
        "id": "6PBSsfBYFyo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All necessary pip installments"
      ],
      "metadata": {
        "id": "ZXA8NQSZFFaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# code here for pip installments\n",
        "!pip install scikit-learn pandas tensorflow keras matplotlib numpy gradio pillow opencv-python"
      ],
      "metadata": {
        "id": "t8iEX0EkDewX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All necessary libraries"
      ],
      "metadata": {
        "id": "fkqQmqQ4FLvC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HA32i4Nxeplg"
      },
      "outputs": [],
      "source": [
        "# code here for libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "from keras.applications import vgg16\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import os\n",
        "import gradio as gr\n",
        "import traceback\n",
        "from keras.layers import Dropout\n",
        "from keras import regularizers\n",
        "from keras.models import load_model\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.stats import mode\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount to Google Drive"
      ],
      "metadata": {
        "id": "s2WpVpqUXYa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pdZqj4jmXX2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 1 - CNN Model from scratch"
      ],
      "metadata": {
        "id": "euP-yKAKer7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Our files:\")\n",
        "!ls /content/drive/MyDrive/KaggleDataset\n",
        "\n",
        "zip_file_path1 = \"/content/drive/MyDrive/KaggleDataset/archive.zip\"\n",
        "zip_file_path2 = \"/content/drive/MyDrive/KaggleDataset/archive2.zip\"\n",
        "\n",
        "# Create directories if they do not exist:\n",
        "!mkdir -p /content/KaggleDataset/archive\n",
        "!mkdir -p /content/KaggleDataset/archive2\n",
        "\n",
        "# Add your own path files here: + unzip\n",
        "!unzip -o $zip_file_path1 -d /content/KaggleDataset\n",
        "!unzip -o $zip_file_path2 -d /content/KaggleDataset\n",
        "\n",
        "# Our data within archive and archive2\n",
        "!ls /content/KaggleDataset/archive/\n",
        "!ls /content/KaggleDataset/archive2/\n",
        "\n",
        "# we will save these paths to not type the path each time\n",
        "path1 = \"/content/KaggleDataset/archive\"\n",
        "path2 = \"/content/KaggleDataset/archive2\""
      ],
      "metadata": {
        "id": "WlIYSfUwbj9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the header of demographics table and store the file in demographic\n",
        "# note the demographics and other tables are in part1 path\n",
        "demographic = pd.read_csv(f\"{path1}/demographic.csv\")\n",
        "print(demographic.head())\n",
        "\n",
        "# Create dictionary to map subject IDs to respective groups healthy or schizo\n",
        "# Load demographic data\n",
        "# format = patient# : 0 /1 [0 = helthy and 1= schizophrenia]\n",
        "diagnosis_dict = dict(zip(demographic.subject, demographic[\" group\"]))\n",
        "print(\"Patient# : 0 / 1\\n\", diagnosis_dict)\n",
        "\n",
        "# Load column labels which correspond to EEG electrodes\n",
        "column_list = pd.read_csv(f\"{path1}/columnLabels.csv\").columns\n",
        "all_elect = list(column_list[4:])\n",
        "print(\"\\nElectrodes List:\\n\",all_elect)\n",
        "\n",
        "# See how many electrodes are there in total\n",
        "print(\"\\nTotal of electrodes used:\\n\", len(all_elect))  # 70\n"
      ],
      "metadata": {
        "id": "jRf2RaJZdSHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize arrays for EEG data and labels\n",
        "N_AVERAGED = 16\n",
        "# x_counter = 0\n",
        "EXPECTED_TRIALS = 100  #  100 trials per patient [change depending on data]\n",
        "max_trials = 81 * EXPECTED_TRIALS  # 81 patients * 100 trials (adjust as needed)\n",
        "input_size = 9216 * len(all_elect) // N_AVERAGED  # Size after averaging\n",
        "\n",
        "X = np.zeros((max_trials, input_size), dtype=\"float32\")\n",
        "Y = np.zeros(max_trials)\n",
        "\n",
        "print(\"Input size:\\n\", input_size)\n",
        "\n",
        "# Function to average rows and reduce dimensionality\n",
        "def avg_rows(a, n):\n",
        "    shape = a.shape\n",
        "    assert len(shape) == 2\n",
        "    assert shape[0] % n == 0\n",
        "    b = a.reshape(shape[0] // n, n, shape[1])\n",
        "    mean_vec = b.mean(axis=1)\n",
        "    return mean_vec"
      ],
      "metadata": {
        "id": "XVd5qlHGdcKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each patient's EEG data\n",
        "x_counter = 0\n",
        "for patient_num in range(1, 81 + 1):\n",
        "    # Check if the path points to a directory\n",
        "    # this part is important in case use of different path files\n",
        "    person_dir_path = os.path.join(path1, f\"{patient_num}.csv\")\n",
        "    if os.path.isdir(person_dir_path):\n",
        "        csv_path = os.path.join(person_dir_path, f\"{patient_num}.csv\")\n",
        "    else:\n",
        "        csv_path = os.path.join(path2, f\"{patient_num}.csv\")\n",
        "        if os.path.isdir(csv_path):\n",
        "            csv_path = os.path.join(csv_path, f\"{patient_num}.csv\")\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, header=None, names=column_list)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found for person number {patient_num} at {csv_path}\")\n",
        "        continue\n",
        "    except IsADirectoryError:\n",
        "        print(f\"Path is a directory, not a file: {csv_path}\")\n",
        "        continue\n",
        "\n",
        "    trials_list = set(df.trial)\n",
        "    valid_trials_count = 0  # To count valid trials for each subject\n",
        "\n",
        "    for trial_number in trials_list:\n",
        "        number_of_trials = len(df[df.trial == trial_number])\n",
        "        if number_of_trials == 9216.0:\n",
        "            valid_trials_count += 1\n",
        "            matrix = df[df.trial == trial_number][all_elect].values\n",
        "            avg_n = avg_rows(matrix, n=N_AVERAGED)\n",
        "            avg_n_vect = avg_n.reshape(-1)\n",
        "            # to see the test for each patient un comment those otherwise it gives a long output\n",
        "            # print(f\"x_counter: {x_counter}, size of X: {X.shape[0]}\")\n",
        "            # print(f\"Shape of avg_n_vect: {avg_n_vect.shape}\")\n",
        "\n",
        "            if x_counter < len(X):\n",
        "                X[x_counter] = avg_n_vect.astype(np.float32)\n",
        "                Y[x_counter] = diagnosis_dict[patient_num]\n",
        "                x_counter += 1\n",
        "            else:\n",
        "                print(f\"Reached limit for array counter {x_counter}\")\n",
        "                break\n",
        "\n",
        "    print(f\"Valid Trials: {valid_trials_count}  ::  For patient number: {patient_num}\")\n"
      ],
      "metadata": {
        "id": "EhxTu0fXdiF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final number of valid trials\n",
        "print(f\"total valid trials: {x_counter}\")\n",
        "\n",
        "# trim unused portion of arrays\n",
        "X = X[:x_counter]\n",
        "Y = Y[:x_counter]\n",
        "\n",
        "print(f\"Final size of X: {X.shape} \") # (7092, 40320)\n",
        "print(f\"Final size of Y: {Y.shape}\") # (7092,)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape data for CNN input\n",
        "\"\"\"\n",
        "7092 data points [labels]\n",
        "each represented by a feature vector of size 40320 (9216 * 92)\n",
        "\"\"\"\n",
        "X_normalized = X_normalized.reshape((X_normalized.shape[0], X_normalized.shape[1], 1))\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "# our input is ready for sequential model !"
      ],
      "metadata": {
        "id": "hAnOwuvEdxf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_normalized.shape[1], 1)),\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dropout(0.25),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train the model & save the training history for plotting\n",
        "history = model.fit(X_normalized, Y, epochs=10, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "E_tm0DUQeVWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_normalized, Y)\n",
        "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "C2GZtMvdeglN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot data to visualize the accuracy / loss\n",
        "# Epoch VS Accuracy\n",
        "plt.figure(figsize=(13, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuraccy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "# Epoch VS Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tqLEUDJOehsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 2 - Three pre-trained models"
      ],
      "metadata": {
        "id": "4SWVDGdQFmLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet50"
      ],
      "metadata": {
        "id": "fYw2uOYtGdsL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BIzmZx2_QCn",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install ssqueezepy\n",
        "!pip install timm\n",
        "!pip install pytorch-lightning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GCMIX4h5_mH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kml3tVv3vWxb"
      },
      "outputs": [],
      "source": [
        "data_aug = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaDjOye9zGHI"
      },
      "outputs": [],
      "source": [
        "class CustomDataGen(tf.keras.utils.Sequence):\n",
        "    def __init__(self, fpath, lbls, bs=32, sz=(224, 224), nch=3, shuf=True, aug=False):\n",
        "        \"\"\"\n",
        "        Initializes the data generator with file paths, labels, and settings.\n",
        "        :param fpath: List of file paths for images.\n",
        "        :param lbls: Corresponding labels for the images.\n",
        "        :param bs: Batch size.\n",
        "        :param sz: Image size (height, width\n",
        "        :param nch: Number of channels\n",
        "        :param shuf: Whether to shuffle data at the end of each epoch.\n",
        "        :param aug: Whether to apply data augmentation.\n",
        "        \"\"\"\n",
        "        self.fpath = fpath\n",
        "        self.lbls = lbls\n",
        "        self.bs = bs\n",
        "        self.sz = sz\n",
        "        self.nch = nch\n",
        "        self.shuf = shuf\n",
        "        self.aug = aug\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of batches per epoch.\"\"\"\n",
        "        return int(np.floor(len(self.fpath) / self.bs))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Generates one batch of data.\n",
        "        :param idx: Index of the batch.\n",
        "        :return: Tuple of (batch of images, batch of labels).\n",
        "        \"\"\"\n",
        "        # Get indexes for the batch\n",
        "        b_idxs = self.idx_arr[idx * self.bs: (idx + 1) * self.bs]\n",
        "        b_fpaths = [self.fpath[i] for i in b_idxs]\n",
        "        b_lbls = [self.lbls[i] for i in b_idxs]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_gen(b_fpaths, b_lbls)\n",
        "        return X, y\n",
        "\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Shuffles the data at the end of each epoch if specified.\"\"\"\n",
        "        self.idx_arr = np.arange(len(self.fpath))\n",
        "        if self.shuf:\n",
        "            np.random.shuffle(self.idx_arr)\n",
        "\n",
        "    def __data_gen(self, b_fpaths, b_lbls):\n",
        "        \"\"\"\n",
        "        Generates data for a batch.\n",
        "        :param b_fpaths: list of file paths for the batch.\n",
        "        :param b_lbls: labels for the batche\n",
        "        :return: Tuple of (batch of images, batch of encoded labels)\n",
        "        \"\"\"\n",
        "        X = np.empty((self.bs, *self.sz, self.nch))\n",
        "        y = np.empty((self.bs), dtype=int)\n",
        "        for i, (fpath, lbl) in enumerate(zip(b_fpaths, b_lbls)):\n",
        "            img = np.load(fpath)\n",
        "            resized_img = np.stack([cv2.resize(ch, self.sz) for ch in img], axis=-1)\n",
        "            resized_img = np.mean(resized_img, axis=-1, keepdims=True)\n",
        "            resized_img = np.repeat(resized_img, self.nch, axis=-1)\n",
        "\n",
        "            if self.aug:\n",
        "                resized_img = data_aug.random_transform(resized_img)\n",
        "\n",
        "            X[i] = resized_img\n",
        "            y[i] = lbl\n",
        "        return X, tf.keras.utils.to_categorical(y, num_classes=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rU61X1_tbF1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# directories containing images for each class yes and noe\n",
        "yes_dir = '/content/drive/MyDrive/KaggleDataset/scaleograms/yes'\n",
        "no_dir = '/content/drive/MyDrive/KaggleDataset/scaleograms/no'\n",
        "\n",
        "# file paths for each class\n",
        "yes_files = [os.path.join(yes_dir, f) for f in os.listdir(yes_dir)]\n",
        "no_files = [os.path.join(no_dir, f) for f in os.listdir(no_dir)]\n",
        "\n",
        "# Combine file and create labels\n",
        "fpath = yes_files + no_files\n",
        "lbls = [1] * len(yes_files) + [0] * len(no_files)\n",
        "\n",
        "# Split data into training and testing\n",
        "train_fpaths, test_fpaths, train_lbls, test_lbls = train_test_split(fpath, lbls, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialze data generator\n",
        "batch_size = 32\n",
        "train_gen = CustomDataGen(train_fpaths, train_lbls, bs=batch_size, aug=True)\n",
        "test_gen = CustomDataGen(test_fpaths, test_lbls, bs=batch_size, aug=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Dg0HJWuteHw",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "\n",
        "# Define and compile the modele\n",
        "img_rows, img_cols = 224, 224\n",
        "resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "moGoPiLmzGHJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Freeze all layers of the ResNet model to prevent their weights from being updated\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the custom top layers to add on top of the base ResNet50 model\n",
        "def top(bottom_model, num_classes):\n",
        "    # Get the output of the base model\n",
        "    top_model = bottom_model.output\n",
        "\n",
        "    # Apply global avg pooling to reduce spatial dimensions\n",
        "    top_model = GlobalAveragePooling2D()(top_model)\n",
        "\n",
        "    #dropout layer with 50% dropout rate to prevent overfitting\n",
        "    top_model = Dropout(0.5)(top_model)\n",
        "\n",
        "    # Add a Dense layer and ReLU activation\n",
        "    top_model = Dense(1024, activation='relu')(top_model)\n",
        "\n",
        "    # Add another Dropout layer\n",
        "    top_model = Dropout(0.5)(top_model)\n",
        "\n",
        "    # Add another Dense layer with and ReLU activaation\n",
        "    top_model = Dense(1024, activation='relu')(top_model)\n",
        "\n",
        "    # another Dropout\n",
        "    top_model = Dropout(0.5)(top_model)\n",
        "\n",
        "    # Add Dense layer with and ReLU\n",
        "    top_model = Dense(512, activation='relu')(top_model)\n",
        "\n",
        "    # Add the final Dense layer with units equal to the number of classes and softmax activation for classification\n",
        "    top_model = Dense(num_classes, activation='softmax')(top_model)\n",
        "\n",
        "    return top_model\n",
        "\n",
        "# Number of output classes for classification\n",
        "num_classes = 2\n",
        "\n",
        "# add the custom top layers to the base ResNet model\n",
        "top_layer = top(resnet, num_classes)\n",
        "\n",
        "# create a new model combining the base ResNet and the custom top layers\n",
        "model = Model(inputs=resnet.input, outputs=top_layer)\n",
        "\n",
        "# Compile the model with Adam optimizer, categorical cross-entropy loss, and accuracy as the evaluation metric\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDs61zsBttXB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define a learning rate scheduler\n",
        "def scheduler(epoch, lr):\n",
        "  # Keep learning rate constant for the first 3 epoch after, decay it exponentially\n",
        "    if epoch < 3:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "\n",
        "# Early stopping callback to halt training when no improvement in validation loss\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Model checkpoint callback to save the best model based on validation loss\n",
        "model_check = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AFhOOtpPWmX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=test_gen,\n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stop]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resnet50 Gradio\n"
      ],
      "metadata": {
        "id": "nCA2yHSdy9gR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPmUhorvt5wg"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "# Load the fine-tuned model\n",
        "mdl = load_model('/content/drive/MyDrive/KaggleDataset/Models/Resnet50_best_model.keras')\n",
        "\n",
        "def pred(file):\n",
        "    try:\n",
        "        # Load and preprocess image data\n",
        "        data = np.load(file.name)\n",
        "        if data.ndim == 3:\n",
        "            # Resize and prepare image\n",
        "            img = np.stack([cv2.resize(ch, (224, 224)) for ch in data], axis=-1)\n",
        "            img = np.mean(img, axis=-1, keepdims=True)\n",
        "            img = np.repeat(img, 3, axis=-1)\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "\n",
        "            # Predict and get class label\n",
        "            pred = mdl.pred(img)\n",
        "            idx = np.argmax(pred, axis=1)[0]\n",
        "            conf = pred[0][idx]\n",
        "            label = ['Healthy', 'Schizophrenia'][idx]\n",
        "\n",
        "            return label, conf\n",
        "        else:\n",
        "            return \"Invalid data shape\", 0.0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error during prediction:\", str(e))\n",
        "        traceback.print_exc()\n",
        "        return \"Error during prediction\", 0.0\n",
        "#check error for loading\n",
        "\n",
        "# Define the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=pred,                             # Function to call\n",
        "    inputs=gr.File(label=\"Upload .npy File\"), # File input\n",
        "    outputs=[gr.Label(num_top_classes=2), gr.Textbox()], #  label and confidence\n",
        "    title=\"Schizophrenia Detection\",\n",
        "    description=\"Upload a .npy file and get the prediction.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(debug=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "InceptionV3"
      ],
      "metadata": {
        "id": "BvgweYzwGlav"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5kWVBN82TMF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
        "import gradio as gr\n",
        "import traceback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJWvEfch2TMH"
      },
      "outputs": [],
      "source": [
        "# Directories\n",
        "yes_dir = '/content/drive/MyDrive/KaggleDataset/scaleograms/yes'\n",
        "no_dir = '/content/drive/MyDrive/KaggleDataset/scaleograms/no'\n",
        "\n",
        "# Custom data generator class inheriting from tf.keras.utils.Sequence\n",
        "class CustomDataGen(tf.keras.utils.Sequence):\n",
        "    def __init__(self, f_paths, labels, b_size=32, img_size=(299, 299), n_chals=3, shuffle=True):\n",
        "        # Initialization method\n",
        "        self.f_paths = f_paths  # file paths\n",
        "        self.labels = labels  #labels\n",
        "        self.b_size = b_size  # Batch size\n",
        "        self.img_size = img_size  # Image size\n",
        "        self.n_chals = n_chals  # nnumber of channels\n",
        "        self.shuffle = shuffle  # Shuffle or no\n",
        "        self.epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the number of batches per epoch\n",
        "        return int(np.floor(len(self.f_paths) / self.b_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate one batch of data\n",
        "        indxs = self.indxs[index * self.b_size: (index + 1) * self.b_size]\n",
        "        f_paths_temp = [self.f_paths[k] for k in indxs]  # temp list of file paths for this batch\n",
        "        labels_temp = [self.labels[k] for k in indxs]  #  labels for this batch\n",
        "\n",
        "        X, y = self._data_gen(f_paths_temp, labels_temp)  # Gnenerate data\n",
        "        return X, y\n",
        "\n",
        "    def epoch_end(self):\n",
        "        # Updates indices after each epoch\n",
        "        self.indxs = np.arange(len(self.f_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indxs)\n",
        "\n",
        "    def _data_gen(self, f_paths_temp, labels_temp):\n",
        "        # Generates data containing batch_size samples **migh need to change\n",
        "        X = np.empty((self.b_size, *self.img_size, self.n_chals))\n",
        "        y = np.empty((self.b_size), dtype=int)\n",
        "        for i, (f_paths, label) in enumerate(zip(f_paths_temp, labels_temp)):\n",
        "            img = np.load(f_paths)\n",
        "            resized = np.stack([cv2.resize(channel, self.img_size) for channel in img], axis=-1)  # Resize image\n",
        "            resized = np.mean(resized, axis=-1, keepdims=True)  # Convert to single channel by averaging\n",
        "            resized = np.repeat(resized, self.n_chals, axis=-1)  # Repeat the single channel to match n_chals\n",
        "            X[i,] = resized\n",
        "            y[i] = label\n",
        "        return X, tf.keras.utils.to_categorical(y, num_classes=2)  # Convert labels to categorical\n",
        "\n",
        "# lis of file paths for yes and no\n",
        "yes_files = [os.path.join(yes_dir, f) for f in os.listdir(yes_dir)]\n",
        "no_files = [os.path.join(no_dir, f) for f in os.listdir(no_dir)]\n",
        "\n",
        "# Combine filepaths and labels\n",
        "f_paths = yes_files + no_files\n",
        "labels = [1] * len(yes_files) + [0] * len(no_files)\n",
        "\n",
        "# split into training and testing sets\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(f_paths, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# batch size\n",
        "b_size = 32\n",
        "\n",
        "# create training and testing data generators\n",
        "train_gen = CustomDataGen(train_paths, train_labels, b_size=b_size)\n",
        "test_gen = CustomDataGen(test_paths, test_labels, b_size=b_size)\n",
        "\n",
        "#  set up the InceptionV3 model\n",
        "img_rows, img_colms = 299, 299\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_rows, img_colms, 3))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "z3E3sHTD2TMH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Freeze the layers of the pre-trained modle\n",
        "for layer in inception.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the new top layers to be added to the pre-trained model\n",
        "def top_(bottom_model, num_classes):\n",
        "    top_m = bottom_model.output\n",
        "    top_m = GlobalAveragePooling2D()(top_m)  # Apply global average pooling\n",
        "    top_m = Dense(1024, activation='relu')(top_m)  # add a dense layer and ReLU activation\n",
        "    top_m = Dense(1024, activation='relu')(top_m)  # add another dense layer with 1024 units and ReLU activation\n",
        "    top_m = Dense(512, activation='relu')(top_m)  # ad a dense layer and ReLU activation\n",
        "    top_m = Dense(num_classes, activation='softmax')(top_m)  # add the final output layer with softmax activation\n",
        "    return top_m\n",
        "\n",
        "# Number of classes\n",
        "num_classes = 2\n",
        "\n",
        "# Create the new model by combining the pre-trained model and the new top layers\n",
        "Top = top_(inception, num_classes)\n",
        "model = Model(inputs=inception.input, outputs=Top)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op6XZ_hd2TMH"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfhDQGoTV5Gi"
      },
      "outputs": [],
      "source": [
        "# Callback\n",
        "\n",
        "# Define a learning rate schedler function\n",
        "def sched(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr  # Keep the learning rate the same for the first 10 epochs\n",
        "    else:\n",
        "        return lr * 0.1  # Reduce the learning rate by a factor of 10 after epoch 10\n",
        "\n",
        "# Early stopping callback to stop trainning when validation loss stops improving\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Learning rate scheduler callback\n",
        "sched = LearningRateScheduler(sched)\n",
        "\n",
        "# Model checkpoint callback to save the best model based on validation loss\n",
        "model_cp = ModelCheckpoint('/content/drive/MyDrive/KaggleDataset/Models/InceptionV3_best_model.keras',\n",
        "                           monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# compile the model with speciied optimizer, loss function, and metrics\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94q_8cf42TMI"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=test_gen,\n",
        "    epochs=20,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stop, sched, model_cp]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inceptionv3 Gradio"
      ],
      "metadata": {
        "id": "4ufzt2Oa2FyS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mPt-MhM-65ej"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "import cv2\n",
        "import traceback\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_path = '/content/drive/MyDrive/KaggleDataset/Models/InceptionV3_best_model.keras'\n",
        "model = load_model(model_path)\n",
        "\n",
        "def process_and_predict(file):\n",
        "    try:\n",
        "        # Load the .npy file\n",
        "        data = np.load(file.name)\n",
        "\n",
        "        if data.ndim == 3:\n",
        "            # Resize and preprocess the image\n",
        "            resized = np.stack([cv2.resize(channel, (299, 299)) for channel in data], axis=-1)\n",
        "            resized = np.mean(resized, axis=-1, keepdims=True)\n",
        "            resized = np.repeat(resized, 3, axis=-1)\n",
        "            resized = np.expand_dims(resized, axis=0)\n",
        "\n",
        "            # Make prediction\n",
        "            pred = model.predict(resized)\n",
        "            idx = np.argmax(pred, axis=1)[0]\n",
        "            confidence = pred[0][idx]\n",
        "            class_label = ['Healthy', 'Schizophrenia']\n",
        "\n",
        "            return class_label[idx], float(confidence)\n",
        "        else:\n",
        "            return \"Invalid data shape\", 0.0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error during prediction:\", str(e))\n",
        "        traceback.print_exc()\n",
        "        return \"Error during prediction\", 0.0\n",
        "\n",
        "# Define the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_and_predict,\n",
        "    inputs=gr.File(label=\"Upload .npy File\"),\n",
        "    outputs=[gr.Label(num_top_classes=2), gr.Textbox()],\n",
        "    title=\"Schizophrenia Detection\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(debug=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG16"
      ],
      "metadata": {
        "id": "Q22jGDkxHMsZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkgsznpzBo08"
      },
      "outputs": [],
      "source": [
        "def get_npy_files(directory):\n",
        "  return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.npy')]\n",
        "\n",
        "#Define paths to scaleograms\n",
        "yes_dir = '/content/drive/MyDrive/KaggleDataset/scaleograms/yes'\n",
        "no_dir = '/content/drive/MyDrive/KaggleDataset/scaleograms/no'\n",
        "\n",
        "yes_files = get_npy_files(yes_dir)\n",
        "no_files = get_npy_files(no_dir)\n",
        "\n",
        "#Custom data generator\n",
        "class CustomDataGen(tf.keras.utils.Sequence):\n",
        "  def __init__(self, file_paths, labels, img_size=(224, 224), n_channels=3, batch_size=32, shuffle=True, repeat_factor=30, **kwargs):\n",
        "    self.original_file_paths = file_paths\n",
        "    self.original_labels = labels\n",
        "    self.batch_size = batch_size\n",
        "    self.img_size = img_size\n",
        "    self.n_channels = n_channels\n",
        "    self.shuffle = shuffle\n",
        "    self.repeat_factor = repeat_factor\n",
        "    self.file_paths, self.labels = self.repeat_data()\n",
        "    self.on_epoch_end()\n",
        "    super(CustomDataGen, self).__init__(**kwargs)\n",
        "\n",
        "  def repeat_data(self):\n",
        "    file_paths_repeated = self.original_file_paths * self.repeat_factor\n",
        "    labels_repeated = self.original_labels * self.repeat_factor\n",
        "    return file_paths_repeated, labels_repeated\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.floor(len(self.file_paths) / self.batch_size))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "    file_paths_temp = [self.file_paths[k] for k in indexes]\n",
        "    labels_temp = [self.labels[k] for k in indexes]\n",
        "    X, y = self.__data_generation(file_paths_temp, labels_temp)\n",
        "    return X, y\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.indexes = np.arange(len(self.file_paths))\n",
        "    if self.shuffle:\n",
        "      np.random.shuffle(self.indexes)\n",
        "\n",
        "  def __data_generation(self, file_paths_temp, labels_temp):\n",
        "    X = np.empty((self.batch_size, *self.img_size, self.n_channels))\n",
        "    y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "    for i, (file_path, label) in enumerate(zip(file_paths_temp, labels_temp)):\n",
        "       img = np.load(file_path)\n",
        "       resized_img = np.stack([cv2.resize(channel, self.img_size) for channel in img], axis=-1)\n",
        "\n",
        "       if resized_img.shape[-1] != self.n_channels:\n",
        "        resized_img = np.mean(resized_img, axis=-1, keepdims=True)\n",
        "        resized_img = np.repeat(resized_img, self.n_channels, axis=-1)\n",
        "\n",
        "       X[i,] = resized_img\n",
        "       y[i] = label\n",
        "\n",
        "    return X, to_categorical(y, num_classes=2)\n",
        "\n",
        "oversample_no_files = random.choices(no_files, k=len(yes_files))\n",
        "file_paths = yes_files + oversample_no_files\n",
        "labels = [1] * len(yes_files) + [0] * len(oversample_no_files)\n",
        "\n",
        "combined = list(zip(file_paths, labels))\n",
        "random.shuffle(combined)\n",
        "file_paths, labels = zip(*combined)\n",
        "file_paths = list(file_paths)\n",
        "labels = list(labels)\n",
        "\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(file_paths, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "train_generator = CustomDataGen(train_paths, train_labels)\n",
        "validation_generator = CustomDataGen(test_paths, test_labels)\n",
        "\n",
        "\n",
        "#Load VGG16 Model\n",
        "img_rows, img_cols = 224, 224\n",
        "vgg = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols, 3))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8ZyX4TvPEWQ"
      },
      "outputs": [],
      "source": [
        "#Add custom layers on top of VGG16\n",
        "\n",
        "def add_custom_layers_with_dropout(base_model, num_classes):\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(num_classes, activation='softmax')(x)\n",
        "  return x\n",
        "\n",
        "num_classes = 2\n",
        "custom_output = add_custom_layers_with_dropout(vgg, num_classes)\n",
        "model_with_dropout = Model(inputs=vgg.input, outputs=custom_output)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ8B6B3ZPT0-",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Phase 1: Train the custom layers with frozen base model layers\n",
        "\n",
        "for layer in vgg.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "model_with_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "checkpoint_phase1 = ModelCheckpoint('best_model_phase1.keras', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, factor=0.5, min_lr=1e-6)\n",
        "\n",
        "history_phase1 = model_with_dropout.fit(train_generator, steps_per_epoch=len(train_paths) // 32, validation_data=validation_generator, validation_steps =len(test_paths) // 32, epochs=10, verbose=1, callbacks=[checkpoint_phase1, early_stopping, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Phase 2: Unfreeze some layers in base network and jointly train\n",
        "for layer in vgg.layers[-4:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "model_with_dropout.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint_phase2 = tf.keras.callbacks.ModelCheckpoint('best_model_finetune.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
        "\n",
        "history_phase2 = model_with_dropout.fit(train_generator, steps_per_epoch=len(train_paths) // 32, validation_data=validation_generator, validation_steps=len(test_paths) // 32, epochs=20, verbose=1, callbacks=[checkpoint_phase2, early_stopping, reduce_lr])\n",
        "\n"
      ],
      "metadata": {
        "id": "BHQJP6-1ho2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKRMgd6Hh-LD"
      },
      "outputs": [],
      "source": [
        "#Plot training and validation accuracy and loss\n",
        "\n",
        "def plot_training_history(history, phase):\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "  plt.title('Training and validation accuracy - {phase 1}')\n",
        "  plt.legend(loc=0)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss - {phase 2}')\n",
        "  plt.legend(loc=0)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plot_training_history(history_phase1, 'Phase 1')\n",
        "plot_training_history(history_phase2, 'Phase 2')\n",
        "\n",
        "model_with_dropout.save('best_model_finetune.keras')\n",
        "\n",
        "best_model = load_model('best_model_finetune.keras')\n",
        "\n",
        "vallidation_generator = CustomDataGen(test_paths, test_labels, img_size=(224, 224), n_channels=3, batch_size=32, shuffle=False)\n",
        "\n",
        "loss, accuracy = best_model.evaluate(validation_generator)\n",
        "print('Test accuracy:', accuracy)\n",
        "print('Test Loss: ', loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG16 Gradio"
      ],
      "metadata": {
        "id": "sBxW-hcOHPZe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZmh6Vk2of4G",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Load the fine-tuned model\n",
        "model = load_model('/content/drive/MyDrive/KaggleDataset/best_model_finetune.keras')\n",
        "\n",
        "def process_and_predict(file):\n",
        "  try:\n",
        "    data = np.load(file.name)\n",
        "\n",
        "    if data.ndim == 3:\n",
        "      resized_img = np.stack([cv2.resize(channel, (224, 224)) for channel in data], axis=-1)\n",
        "      resized_img = np.mean(resized_img, axis=-1, keepdims=True)\n",
        "      resized_img = np.repeat(resized_img, 3, axis=-1)\n",
        "      resized_img = np.expand_dims(resized_img, axis=0)\n",
        "\n",
        "\n",
        "      prediction = model.predict(resized_img)\n",
        "      class_idx = np.argmax(prediction, axis=1)[0]\n",
        "      confidence = prediction[0][class_idx]\n",
        "      class_label = ['Healthy', 'Schizophrenia'][class_idx]\n",
        "\n",
        "      return class_label, confidence\n",
        "    else:\n",
        "      return \"Invalid data shape\", 0.0\n",
        "\n",
        "  except Exception as e:\n",
        "    print(\"Error during prediction:\", str(e))\n",
        "    traceback.print_exc()\n",
        "    return \"Error during prediction\", 0.0\n",
        "\n",
        "iface = gr.Interface(\n",
        "  fn=process_and_predict,\n",
        "  inputs=gr.File(label=\"Upload .npy File\"),\n",
        "  outputs=[gr.Label(num_top_classes=2), gr.Textbox()],\n",
        "  title=\"Schizophrenia Detection\",\n",
        "  description=\"Upload a .npy file and get the prediction.\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble Model + Gradio"
      ],
      "metadata": {
        "id": "R3MAEK5lHsax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_npy_files(file_paths, img_size):\n",
        "  data = []\n",
        "  labels = []\n",
        "  for file_path in file_paths:\n",
        "    label = 1 if 'yes' in file_path else 0\n",
        "    img = np.load(file_path)\n",
        "    resized_img = np.stack([cv2.resize(channel, img_size) for channel in img], axis=-1)\n",
        "    if resized_img.shape[-1] != 3:\n",
        "      resized_img = np.mean(resized_img, axis=-1, keepdims=True)\n",
        "      resized_img = np.repeat(resized_img, 3, axis=-1)\n",
        "    data.append(resized_img)\n",
        "    labels.append(label)\n",
        "\n",
        "  # Convert lists to numpy array\n",
        "  data = np.array(data)\n",
        "  labels = np.array(labels)\n",
        "  return data, labels\n",
        "\n",
        "  #Directories for 'yes' and 'no' scaleograms\n",
        "\n",
        "yes_dir = '/content/drive/MyDrive/KaggleDataset/scaleograms/yes'\n",
        "no_dir = '/content/drive/MyDrive/KaggleDataset/scaleograms/no'\n",
        "\n",
        "yes_files = [os.path.join(yes_dir, f) for f in os.listdir(yes_dir) if f.endswith('.npy')]\n",
        "no_files = [os.path.join(no_dir, f) for f in os.listdir(no_dir) if f.endswith('.npy')]\n",
        "\n",
        "\n",
        "#Oversample the 'no' class to balance the dataset\n",
        "oversampled_no_files = random.choices(no_files, k=len(yes_files))\n",
        "all_files = yes_files + oversampled_no_files\n",
        "labels = [1] * len(yes_files) + [0] * len(oversampled_no_files)\n",
        "\n",
        "#Shuffle the combined dataset\n",
        "combined = list(zip(all_files, labels))\n",
        "random.shuffle(combined)\n",
        "all_files, labels = zip(*combined)\n",
        "all_files = list(all_files)\n",
        "labels = list(labels)\n",
        "\n",
        "#Split the dataset into training and testing sets\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(all_files, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the pre-trained models\n",
        "vgg16_model = load_model('/content/drive/MyDrive/KaggleDataset/best_model_finetune.keras')\n",
        "resnet50_model = load_model('/content/drive/MyDrive/KaggleDataset/Resnet50_best_model.keras')\n",
        "inceptionv3_model = load_model('/content/drive/MyDrive/KaggleDataset/InceptionV3_best_model.keras')\n",
        "\n",
        "def preprocess_image_for_model(img, model_name):\n",
        "  if model_name == 'inceptionv3':\n",
        "    img = cv2.resize(img, (299, 299))\n",
        "  else:\n",
        "      img_size = (224, 224)\n",
        "  resized_img = np.stack([cv2.resize(channel, img_size) for channel in img], axis=-1)\n",
        "  if resized_img.shape[-1] != 3:\n",
        "    resized_img = np.mean(resized_img, axis=-1, keepdims=True)\n",
        "    resized_img = np.repeat(resized_img, 3, axis=-1)\n",
        "  return resized_img\n",
        "\n",
        "# Preprocess test data for each model\n",
        "X_test_vgg16, y_test = load_and_preprocess_npy_files(test_paths, (224, 224))\n",
        "x_test_resnet50, _ = load_and_preprocess_npy_files(test_paths, (224, 224))\n",
        "x_test_inceptionv3, _ = load_and_preprocess_npy_files(test_paths, (299, 299))\n",
        "\n",
        "# Get predictions from each model\n",
        "vgg16_preds = vgg16_model.predict(X_test_vgg16)\n",
        "resnet50_preds = resnet50_model.predict(x_test_resnet50)\n",
        "inceptionv3_preds = inceptionv3_model.predict(x_test_inceptionv3)\n",
        "\n",
        "#Majority voting for ensemble prediction\n",
        "def majority_voting(preds):\n",
        "  preds = np.array(preds)\n",
        "  majority_voting = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=preds)\n",
        "  return majority_voting\n",
        "\n",
        "ensemble_preds = majority_voting([np.argmax(vgg16_preds, axis=1), np.argmax(resnet50_preds, axis=1), np.argmax(inceptionv3_preds, axis=1)])\n",
        "accuracy = np.mean(ensemble_preds == y_test)\n",
        "\n",
        "def process_and_predict(file):\n",
        "  try:\n",
        "    data = np.load(file.name)\n",
        "    #Check if data has 3 dimensions\n",
        "    if data.ndim == 3:\n",
        "      resized_img_vgg16 = preprocess_image_for_model(data, 'vgg16')\n",
        "      resized_img_resnet50 = preprocess_image_for_model(data, 'resnet50')\n",
        "      resized_img_inceptionv3 = preprocess_image_for_model(data, 'inceptionv3')\n",
        "\n",
        "      #Get predictions from each model\n",
        "      vgg16_pred = vgg16_model.predict(np.expand_dims(resized_img_vgg16, axis=0))\n",
        "      resnet50_pred = resnet50_model.predict(np.expand_dims(resized_img_resnet50, axis=0))\n",
        "      inceptionv3_pred = inceptionv3_model.predict(np.expand_dims(resized_img_inceptionv3, axis=0))\n",
        "\n",
        "      majority_vote = majority_voting([np.argmax(vgg16_pred, axis=1), np.argmax(resnet50_pred, axis=1), np.argmax(inceptionv3_pred, axis=1)])\n",
        "      class_idx = majority_vote[0]\n",
        "      confidence = max(vgg16_pred[0][class_idx], resnet50_pred[0][class_idx], inceptionv3_pred[0][class_idx])\n",
        "      class_label = ['Healthy', 'Schizophrenia'][class_idx]\n",
        "\n",
        "      return class_label, confidence\n",
        "    else:\n",
        "      return \"Invalid data shape\", 0.0\n",
        "\n",
        "  except Exception as e:\n",
        "    print(\"Error during prediction:\", str(e))\n",
        "    traceback.print_exc()\n",
        "    return \"Error during prediction\", 0.0\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_and_predict,\n",
        "    inputs=gr.File(label=\"Upload .npy file\"),\n",
        "    outputs=[gr.Label(num_top_classes=2), gr.Textbox()],\n",
        "    title=\"Schizophrenia Detection Ensemble Model\",\n",
        "    description=\"Upload a .npy file for prediction.\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6RO5l5MoHpy5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}